{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression\n",
    "\n",
    "y = mx + b\n",
    "y = wx + b\n",
    "\n",
    "w-weight   b - bias\n",
    "\n",
    "augmented X = [1,X.....]\n",
    "theta = [b,W.......]\n",
    "note- if u add b first then augment 1 first. If u add b last then augment b last.\n",
    "If there is no bias there is no augmentation required.\n",
    "y = theta * augmented X \n",
    "This is Dot product\n",
    "Augmented X is transposed X.T\n",
    "\n",
    "Its like y = w1x1 + w2x2 + b*1 \n",
    "\n",
    "MSE = (y pred - y actual)^2\n",
    "MSE = (theta * augmented X.T - y actual) ^ 2\n",
    "\n",
    "loss function \n",
    "1/m * sum of MSE\n",
    "\n",
    "gradient\n",
    "2/m * sum of ( (theta * augmented X.T - y actual) * augmented X)\n",
    "2/5 * (w.dot(x.T) - y).dot(x)\n",
    "\n",
    "gradient descent\n",
    "w = w - .1 * (2/5 * (w.dot(x.T) - y).dot(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic commands\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = SelectKBest(f_regression, k=5) #k is number of features.\n",
    "X_train_sel = best.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#('preparation', ......stand),\n",
    "#SKLearn's linear regression pipeline\n",
    "featureScores = feature_importances[[\"F Score\"]].values.flatten()\n",
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        ('feature_selection', TopFeatureSelector(featureScores, k)),\n",
    "        ('lineReg', LinearRegression())\n",
    "    ])\n",
    "#print(preparation_and_feature_selection_pipeline)\n",
    "preparation_and_feature_selection_pipeline.fit(X_train.values, y_train) \n",
    "predictions = preparation_and_feature_selection_pipeline.predict(some_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "# remove rows that contain missing values\n",
    "df.dropna(axis=0)\n",
    "\n",
    "# only drop rows where all columns are NaN\n",
    "df.dropna(how='all')  \n",
    "\n",
    "# drop rows that have less than 3 real values \n",
    "df.dropna(thresh=4)\n",
    "\n",
    "# only drop rows where NaN appear in specific columns (here: 'C')\n",
    "df.dropna(subset=['C'])\n",
    "\n",
    "# impute missing values via the column mean\n",
    "from sklearn.preprocessing import Imputer\n",
    "imr = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Label encoding with sklearn's LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "ohe.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mapping = {'XL': 3,\n",
    "                'L': 2,\n",
    "                'M': 1}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df\n",
    "\n",
    "inv_size_mapping = {v: k for k, v in size_mapping.items()}\n",
    "df['size'].map(inv_size_mapping)\n",
    "\n",
    "import numpy as np\n",
    "# create a mapping dict\n",
    "# to convert class labels from strings to integers\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['classlabel']))}\n",
    "class_mapping\n",
    "\n",
    "# to convert class labels from strings to integers\n",
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.array([1, 5, 1, 2])\n",
    "w=np.array([2,-1, 4, 8])\n",
    "eta = .01\n",
    "y = 6\n",
    "w=w-eta*np.dot((np.dot(X,w)-y),X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 = LASSO\n",
    "L2 = Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 Lasso regularization, \n",
    "# note - np.append, 0 and weight vector from [1:], bias is not considered for mixing coefficient here\n",
    "w = w - eta * (np.dot((np.dot(X,w)-y),X) + np.append(0,alpha*np.sign(w[1:]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](snap3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "# learn the 'vocabulary' of the training data (occurs in-place)\n",
    "vect.fit(simple_train)\n",
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()\n",
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm\n",
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()\n",
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- `vect.fit(train)` **learns the vocabulary** of the training data\n",
    "- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n",
    "- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data (and **ignores tokens** it hasn't seen before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'ham':0, 'spam':1})\n",
    "\n",
    "# convert text target classes to numerical target classes\n",
    "lb = LabelBinarizer()\n",
    "sms['label'] = lb.fit_transform(sms['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test case\n",
    "x = np.array([0., 1.])\n",
    "#Model W: 3 hyperplane normals\n",
    "W = np.transpose(np.array([[1., 1.], \n",
    "[-4., 3.], \n",
    "[-1., -3.]])).reshape(2, 3)\n",
    "#Model bias terms each class hyperplane\n",
    "b=np.array([-2., -11.,-1.])\n",
    "np.round(np.dot(x, W)+b) # STEP 1: perpendicular distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test case\n",
    "x = np.array([0., 1.])\n",
    "#Model W: 3 hyperplane normals\n",
    "W = np.transpose(np.array([[1., 1.], \n",
    "[-4., 3.], \n",
    "[-1., -3.]])).reshape(2, 3)\n",
    "#Model bias terms each class hyperplane\n",
    "b=np.array([-2., -11.,-1.])\n",
    "def probas(W, b, x):\n",
    "    sc = np.dot(x, W)+b # STEP 1: perpendicular distances\n",
    "    sc_exp = np.exp(sc) # STEP 2: make all distances positive\n",
    "    return sc_exp / np.sum(sc_exp) # STEP 3: normalize to get probabilities\n",
    "np.round(probas(W, b, x), 3) #2 decimal places"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
